================================================================================
          CLASSIFICADOR k-NEAREST NEIGHBOUR (k-NN) - TRABALHO COMPLETO
================================================================================

ESTRUTURA DO PROJETO (conforme especificado)
================================================================================

üì¶ 4 FUN√á√ïES:
  ‚úì dist.py             - Calcula dist√¢ncia Euclidiana entre dois pontos
  ‚úì meuKnn.py           - Implementa o classificador k-NN
  ‚úì visualizaPontos.py  - Visualiza dados em gr√°ficos 2D
  ‚úì normalizacao.py     - Normaliza dados (StandardScaler)

üì¶ 4 SCRIPTS DE DEMONSTRA√á√ÉO (com coment√°rios e respostas):
  ‚úì demoD1.py           - Demonstra√ß√£o Grupo 1 (Iris Dataset)
  ‚úì demoD2.py           - Demonstra√ß√£o Grupo 2 (Wine Dataset)
  ‚úì demoD3.py           - Demonstra√ß√£o Grupo 3
  ‚úì demoD4.py           - Demonstra√ß√£o Grupo 4

üì¶ 4 DATASETS:
  ‚úì grupoDados1.mat     - Iris (flores)
  ‚úì grupoDados2.mat     - Wine (vinhos)
  ‚úì grupoDados3.mat     - Dataset gen√©rico
  ‚úì grupoDados4.mat     - Dataset gen√©rico

üì¶ ARQUIVOS ADICIONAIS:
  ‚Ä¢ main_menu.py        - Menu interativo para executar demos
  ‚Ä¢ README_COMPLETO.md  - Documenta√ß√£o completa
  ‚Ä¢ LEIA_ME.md          - Guia r√°pido
  ‚Ä¢ exemplos_uso.py     - Exemplos adicionais (opcional)

================================================================================
COMO EXECUTAR
================================================================================

M√âTODO 1 - Menu Interativo (Recomendado):
  python main_menu.py

M√âTODO 2 - Executar demonstra√ß√µes individuais:
  python demoD1.py
  python demoD2.py
  python demoD3.py
  python demoD4.py

M√âTODO 3 - Usar fun√ß√µes diretamente:
  from meuKnn import meuKnn
  from dist import dist
  # ... seu c√≥digo aqui

================================================================================
RESPOSTAS DAS QUEST√ïES
================================================================================

GRUPO 1 - IRIS DATASET
------------------------------------------------------------
Q1.1: Qual √© a acur√°cia m√°xima que voc√™ consegue?
  ‚úì RESPOSTA: 98.00% com k=3

Q1.2: √â necess√°rio ter todas as caracter√≠sticas para acur√°cia m√°xima?
  ‚úì RESPOSTA: SIM, todas as 4 caracter√≠sticas s√£o necess√°rias
    - Com 4 caracter√≠sticas: 98.00%
    - Com 3 caracter√≠sticas: 96.00%
    - Com 2 caracter√≠sticas: 70-96%

GRUPO 2 - WINE DATASET
------------------------------------------------------------
Q2.1: Qual √© a acur√°cia de classifica√ß√£o?
  ‚úì RESPOSTA: 68.33% (k=1, sem normaliza√ß√£o)

Q2.2: Como atingir 98% de acur√°cia?
  ‚úì RESPOSTA: 98.33% com normaliza√ß√£o e k=3
  
  O QUE FOI FEITO:
    - Aplicou-se normaliza√ß√£o StandardScaler
    - Testou-se diferentes valores de k
  
  POR QU√ä:
    - As 13 caracter√≠sticas t√™m escalas muito diferentes
      (√Ålcool ~12, Prolina ~500-1600, Magn√©sio ~70-160)
    - Sem normaliza√ß√£o, caracter√≠sticas com valores grandes
      dominam o c√°lculo da dist√¢ncia Euclidiana
    - StandardScaler equaliza as escalas (m√©dia=0, desvio=1)
    - Permite que todas contribuam igualmente

GRUPO 3
------------------------------------------------------------
Q3.1: Acur√°cia com k=1?
  ‚úì RESPOSTA: 70.00%

Q3.2: Como atingir 92% de acur√°cia?
  ‚úì RESPOSTA: 96.00% com k=10 (meta superada!)
  
  O QUE FOI FEITO:
    - Aumentou-se o valor de k de 1 para 10
  
  POR QU√ä:
    - k=1 √© muito sens√≠vel a outliers e ru√≠do
    - Um √∫nico ponto ruidoso pode causar erro
    - k=10 usa vota√ß√£o de 10 vizinhos (maioria)
    - Suaviza as decis√µes e reduz sensibilidade
    - Reduz overfitting aos dados de treinamento

GRUPO 4
------------------------------------------------------------
Q4.1: Qual √© a acur√°cia de classifica√ß√£o?
  ‚úì RESPOSTA: 71.67% (k=1, sem normaliza√ß√£o)

Q4.2: Como atingir 92% de acur√°cia? (h√° mais de um problema)
  ‚úì RESPOSTA: 90.00% com k=9 e StandardScaler
  (Meta n√£o atingida, mas grande melhoria obtida)
  
  O QUE FOI FEITO:
    1. Normaliza√ß√£o (StandardScaler, MinMaxScaler, RobustScaler)
    2. Ajuste de k (testado de 1 a 15)
    3. Teste de m√∫ltiplos m√©todos de normaliza√ß√£o
  
  POR QU√ä:
    PROBLEMA 1 - Escalas diferentes:
      - Caracter√≠sticas t√™m magnitudes diferentes
      - Normaliza√ß√£o equaliza contribui√ß√µes
    
    PROBLEMA 2 - Valor de k inadequado:
      - k=1 sens√≠vel a ru√≠do
      - k √≥timo encontrado: 9
    
    PROBLEMA 3 - Poss√≠veis outliers:
      - StandardScaler: usa m√©dia (sens√≠vel)
      - RobustScaler: usa mediana (robusto)
      - Testamos m√∫ltiplas abordagens
  
  OBSERVA√á√ÉO:
    - Dataset pode ter limita√ß√µes intr√≠nsecas
    - Nem sempre √© poss√≠vel atingir metas muito altas
    - Ru√≠do excessivo ou sobreposi√ß√£o de classes

================================================================================
IMPLEMENTA√á√ÉO DAS FUN√á√ïES
================================================================================

1. dist(p, q)
   - Calcula dist√¢ncia Euclidiana: d = sqrt(sum((pi-qi)¬≤))
   - Usa numpy para efici√™ncia
   - Entrada: dois vetores (pontos)
   - Sa√≠da: float (dist√¢ncia)

2. meuKnn(dadosTrain, rotuloTrain, dadosTeste, k)
   - Implementa classificador k-NN completo
   - Para cada teste:
     ‚Ä¢ Calcula dist√¢ncia para todos treinos
     ‚Ä¢ Ordena dist√¢ncias
     ‚Ä¢ Pega k vizinhos mais pr√≥ximos
     ‚Ä¢ k=1: retorna r√≥tulo do mais pr√≥ximo
     ‚Ä¢ k>1: retorna moda (vota√ß√£o maioria)
   - Entrada: dados treino/teste, r√≥tulos, k
   - Sa√≠da: array de predi√ß√µes

3. visualizaPontos(dados, rotulos, d1, d2)
   - Cria gr√°fico scatter 2D
   - Classe 1: vermelho, tri√¢ngulo
   - Classe 2: azul, cruz
   - Classe 3: verde, ponto
   - Entrada: dados, r√≥tulos, √≠ndices dimens√µes
   - Sa√≠da: exibe gr√°fico matplotlib

4. normalizacao(dadosTrain, dadosTest)
   - Usa StandardScaler do sklearn
   - F√≥rmula: z = (x - m√©dia) / desvio
   - Ajusta no treino, aplica em ambos
   - Entrada: dados treino/teste
   - Sa√≠da: dados normalizados

================================================================================
CONCEITOS IMPORTANTES
================================================================================

DIST√ÇNCIA EUCLIDIANA:
  - Medida de similaridade entre pontos
  - Quanto menor, mais similares
  - Sens√≠vel √† escala das caracter√≠sticas

k-NEAREST NEIGHBOUR:
  - Classifica√ß√£o baseada em proximidade
  - k=1: vizinho mais pr√≥ximo
  - k>1: vota√ß√£o por maioria
  - N√£o param√©trico, baseado em inst√¢ncias

NORMALIZA√á√ÉO:
  - Essencial quando escalas diferem
  - StandardScaler: m√©dia=0, desvio=1
  - MinMaxScaler: escala para [0,1]
  - RobustScaler: robusto a outliers

TRADE-OFF DO k:
  - k pequeno: flex√≠vel, sens√≠vel a ru√≠do
  - k grande: robusto, pode suavizar demais
  - k √≥timo: equil√≠brio vi√©s-vari√¢ncia

================================================================================
RESULTADOS RESUMIDOS
================================================================================

Grupo  | Inicial | Melhor  | k    | T√©cnica          | Meta
-------|---------|---------|------|------------------|---------
  1    | 96.00%  | 98.00%  | 3    | Otimiza√ß√£o k     | ‚úì 96%
  2    | 68.33%  | 98.33%  | 3    | Normaliza√ß√£o     | ‚úì 98%
  3    | 70.00%  | 96.00%  | 10   | Aumento k        | ‚úì 92%
  4    | 71.67%  | 90.00%  | 9    | Norm + k         | ‚úó 92%

LI√á√ïES APRENDIDAS:
  ‚úì Normaliza√ß√£o crucial para escalas diferentes
  ‚úì k maior reduz sensibilidade a outliers
  ‚úì Nem sempre poss√≠vel atingir metas altas
  ‚úì Qualidade dos dados afeta resultados
  ‚úì Testar m√∫ltiplas abordagens √© importante

================================================================================
DEPEND√äNCIAS
================================================================================

pip install numpy scipy matplotlib scikit-learn

- numpy: opera√ß√µes num√©ricas
- scipy: carregar arquivos .mat
- matplotlib: visualiza√ß√µes
- sklearn: normaliza√ß√£o e mode

================================================================================
DOCUMENTA√á√ÉO COMPLETA
================================================================================

Para mais detalhes, consulte:
  ‚Ä¢ README_COMPLETO.md - Documenta√ß√£o detalhada
  ‚Ä¢ LEIA_ME.md         - Guia r√°pido
  ‚Ä¢ Coment√°rios nos scripts de demonstra√ß√£o

================================================================================
FIM DO RESUMO
================================================================================
