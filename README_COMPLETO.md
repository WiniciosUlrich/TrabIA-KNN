# Classificador k-Nearest Neighbour (k-NN)

Este projeto implementa um classificador k-Nearest Neighbour (k-NN) do zero usando Python e testa em quatro grupos de dados diferentes.

## üìã Descri√ß√£o

O classificador k-NN √© um dos algoritmos de classifica√ß√£o mais simples e intuitivos. Ele classifica um novo exemplo baseado nos k exemplos de treinamento mais pr√≥ximos usando dist√¢ncia Euclidiana.

## üìÅ Estrutura do Projeto

### Fun√ß√µes Principais (conforme especifica√ß√£o):
- **`dist.py`** - Fun√ß√£o para calcular dist√¢ncia Euclidiana entre dois pontos
- **`meuKnn.py`** - Implementa√ß√£o do classificador k-NN
- **`visualizaPontos.py`** - Fun√ß√µes para visualizar dados em gr√°ficos 2D
- **`normalizacao.py`** - Fun√ß√£o para normalizar dados (StandardScaler)

### Scripts de Demonstra√ß√£o (conforme especifica√ß√£o):
- **`demoD1.py`** - Demonstra√ß√£o com Grupo de Dados 1 (Iris Dataset)
- **`demoD2.py`** - Demonstra√ß√£o com Grupo de Dados 2 (Wine Dataset)
- **`demoD3.py`** - Demonstra√ß√£o com Grupo de Dados 3
- **`demoD4.py`** - Demonstra√ß√£o com Grupo de Dados 4

### Scripts Auxiliares:
- **`main_menu.py`** - Menu interativo para executar as demonstra√ß√µes
- **`main.py`** - Script de an√°lise completa integrada (opcional)
- **`exemplos_uso.py`** - Exemplos adicionais de uso (opcional)

### Datasets:
- `grupoDados1.mat` - Dataset Iris (flores)
- `grupoDados2.mat` - Dataset Wine (vinhos)
- `grupoDados3.mat` - Dataset gen√©rico
- `grupoDados4.mat` - Dataset gen√©rico

## üöÄ Como Usar

### Op√ß√£o 1: Menu Interativo (Recomendado)
```bash
python main_menu.py
```
Escolha qual demonstra√ß√£o deseja executar ou execute todas em sequ√™ncia.

### Op√ß√£o 2: Executar Demonstra√ß√µes Individualmente
```bash
# Grupo 1 - Iris Dataset
python demoD1.py

# Grupo 2 - Wine Dataset
python demoD2.py

# Grupo 3
python demoD3.py

# Grupo 4
python demoD4.py
```

### Op√ß√£o 3: Usar as Fun√ß√µes Diretamente
```python
from meuKnn import meuKnn
from dist import dist
import scipy.io as scipy
import numpy as np

# Carregar dados
mat = scipy.loadmat('grupoDados1.mat')
grupo_test = mat['grupoTest']
grupo_train = mat['grupoTrain']
test_rots = mat['testRots'].flatten()
train_rots = mat['trainRots'].flatten()

# Fazer predi√ß√£o com k=3
predicoes = meuKnn(grupo_train, train_rots, grupo_test, k=3)

# Calcular acur√°cia
acuracia = np.sum(predicoes == test_rots) / len(test_rots)
print(f"Acur√°cia: {acuracia:.4f} ({acuracia*100:.2f}%)")
```

## üìä Resultados Obtidos (Respostas √†s Quest√µes)

### Grupo 1 (Iris Dataset)
- **Q1.1:** Acur√°cia m√°xima = **98.00%** (k=3)
- **Q1.2:** SIM, todas as 4 caracter√≠sticas s√£o necess√°rias para acur√°cia m√°xima
  - Com 4 caracter√≠sticas: 98.00%
  - Com 3 caracter√≠sticas: ~96.00%
  - Com 2 caracter√≠sticas: 70-96%

**Observa√ß√£o:** Caracter√≠sticas 2 e 3 (relacionadas √†s p√©talas) s√£o mais discriminativas que 0 e 1 (s√©palas).

---

### Grupo 2 (Wine Dataset)
- **Q2.1:** Acur√°cia inicial = **68.33%** (k=1, sem normaliza√ß√£o)
- **Q2.2:** Acur√°cia de **98.33%** atingida (k=3, COM normaliza√ß√£o)

**O que foi feito:**
- Aplicou-se normaliza√ß√£o StandardScaler aos dados
- Testou-se diferentes valores de k

**Por qu√™:**
- As 13 caracter√≠sticas qu√≠micas t√™m escalas muito diferentes
  - √Ålcool: ~12-14
  - Prolina: ~500-1600
  - Magn√©sio: ~70-160
- Sem normaliza√ß√£o, caracter√≠sticas com valores maiores dominam a dist√¢ncia Euclidiana
- StandardScaler coloca todas as caracter√≠sticas na mesma escala (m√©dia=0, desvio=1)
- Isso permite que todas contribuam igualmente para a classifica√ß√£o

**Resultado:**
- Melhoria de ~30 pontos percentuais
- ‚úì Meta de 98% atingida!

---

### Grupo 3
- **Q3.1:** Acur√°cia com k=1 = **70.00%**
- **Q3.2:** Acur√°cia de **96.00%** atingida (k=10)

**O que foi feito:**
- Aumentou-se o valor de k de 1 para 10

**Por qu√™:**
- k=1 (1-NN) √© muito sens√≠vel a outliers e ru√≠do
  - Um √∫nico ponto ruidoso pode causar erro
  - N√£o h√° mecanismo de suaviza√ß√£o
- k=10 (k-NN) √© mais robusto
  - Usa vota√ß√£o de 10 vizinhos (vota√ß√£o por maioria)
  - Outliers t√™m menos influ√™ncia
  - Suaviza as fronteiras de decis√£o
  - Reduz overfitting

**Resultado:**
- Melhoria de ~26 pontos percentuais
- ‚úì Meta de 92% atingida (inclusive superada: 96%)!

---

### Grupo 4
- **Q4.1:** Acur√°cia inicial = **71.67%** (k=1, sem normaliza√ß√£o)
- **Q4.2:** M√°xima acur√°cia = **90.00%** (k=9, StandardScaler)

**O que foi feito:**
1. Normaliza√ß√£o dos dados (StandardScaler, MinMaxScaler, RobustScaler)
2. Ajuste do valor de k (testado k de 1 a 15)
3. Teste de m√∫ltiplos m√©todos de normaliza√ß√£o

**Por qu√™:**
- **PROBLEMA 1 - Escalas diferentes:**
  - Caracter√≠sticas t√™m magnitudes diferentes
  - Dist√¢ncia Euclidiana √© sens√≠vel a escala
  - Normaliza√ß√£o equaliza a contribui√ß√£o

- **PROBLEMA 2 - Valor de k inadequado:**
  - k=1 muito sens√≠vel a ru√≠do
  - k muito alto suaviza demais
  - k √≥timo encontrado: 9

- **PROBLEMA 3 - Poss√≠veis outliers:**
  - StandardScaler usa m√©dia (sens√≠vel a outliers)
  - RobustScaler usa mediana (robusto a outliers)
  - MinMaxScaler escala para [0,1]

**Resultado:**
- Melhoria de ~18 pontos percentuais
- ‚ö† Meta de 92% n√£o atingida (m√°ximo: 90.00%)
- Poss√≠veis limita√ß√µes intr√≠nsecas do dataset

---

## üîß Fun√ß√µes Implementadas

### 1. `dist(p, q)`
Calcula a dist√¢ncia Euclidiana entre dois pontos.

```python
from dist import dist
import numpy as np

p = np.array([1, 2, 3])
q = np.array([4, 5, 6])
distancia = dist(p, q)  # Retorna: 5.196...
```

**F√≥rmula:** d(p,q) = ‚àö(Œ£(pi - qi)¬≤)

---

### 2. `meuKnn(dadosTrain, rotuloTrain, dadosTeste, k)`
Implementa o classificador k-NN.

**Par√¢metros:**
- `dadosTrain`: Dados de treinamento (matriz)
- `rotuloTrain`: R√≥tulos dos dados de treinamento
- `dadosTeste`: Dados de teste
- `k`: N√∫mero de vizinhos mais pr√≥ximos

**Retorna:** Array com predi√ß√µes

**Como funciona:**
1. Para cada exemplo de teste
2. Calcula dist√¢ncia para todos os exemplos de treinamento
3. Ordena as dist√¢ncias
4. Pega os k vizinhos mais pr√≥ximos
5. Se k=1: retorna o r√≥tulo do vizinho mais pr√≥ximo
6. Se k>1: retorna a moda (vota√ß√£o) dos k vizinhos

---

### 3. `visualizaPontos(dados, rotulos, d1, d2)`
Visualiza dados em 2D.

**Par√¢metros:**
- `dados`: Matriz de dados
- `rotulos`: Array de r√≥tulos
- `d1, d2`: √çndices das dimens√µes a visualizar

**Exemplo:**
```python
from visualizaPontos import visualizaPontos
import scipy.io as scipy

mat = scipy.loadmat('grupoDados1.mat')
dados = mat['grupoTrain']
rotulos = mat['trainRots'].flatten()

# Visualizar dimens√µes 0 e 1
visualizaPontos(dados, rotulos, 0, 1)
```

---

### 4. `normalizacao(dadosTrain, dadosTest)`
Normaliza os dados usando StandardScaler.

**Por que normalizar?**
- k-NN usa dist√¢ncia Euclidiana
- Caracter√≠sticas com valores grandes dominam
- Normaliza√ß√£o equaliza as escalas

**F√≥rmula:** z = (x - m√©dia) / desvio_padr√£o

**Retorna:** Tupla (dadosTrain_normalizado, dadosTest_normalizado)

---

## üí° Li√ß√µes Aprendidas

1. **Normaliza√ß√£o √© CRUCIAL** para dados com escalas diferentes
   - Wine dataset: 68% ‚Üí 98% com normaliza√ß√£o
   
2. **k=1 √© sens√≠vel a outliers**; k maior suaviza decis√µes
   - Grupo 3: 70% ‚Üí 96% aumentando k
   
3. **Nem sempre √© poss√≠vel atingir metas altas** de acur√°cia
   - Grupo 4: m√°ximo 90% (limita√ß√µes do dataset)
   
4. **Qualidade dos dados** afeta significativamente os resultados
   
5. **Diferentes normaliza√ß√µes** podem ter resultados diferentes
   - StandardScaler, MinMaxScaler, RobustScaler

## üìà Depend√™ncias

```bash
pip install numpy scipy matplotlib scikit-learn
```

**Bibliotecas utilizadas:**
- `numpy` - Opera√ß√µes num√©ricas
- `scipy.io` - Carregar arquivos .mat
- `matplotlib.pyplot` - Visualiza√ß√µes
- `scipy.stats` - Fun√ß√£o mode (moda)
- `sklearn.preprocessing` - Normaliza√ß√£o

## üéØ Conceitos Importantes

### Dist√¢ncia Euclidiana
Medida de similaridade entre dois pontos no espa√ßo.
Quanto menor a dist√¢ncia, mais similares os pontos.

### Vota√ß√£o por Maioria
Para k>1, o r√≥tulo previsto √© aquele que aparece mais vezes entre os k vizinhos.

### Normaliza√ß√£o
Transforma dados para mesma escala, evitando que caracter√≠sticas com valores grandes dominem.

### Trade-off do k
- **k pequeno:** Mais flex√≠vel, mas sens√≠vel a ru√≠do
- **k grande:** Mais robusto, mas pode suavizar demais
- **k √≥timo:** Equil√≠brio entre vi√©s e vari√¢ncia

## üìù Estrutura dos Coment√°rios nos Scripts

Cada script de demonstra√ß√£o (`demoD1.py`, `demoD2.py`, etc.) cont√©m:
- Descri√ß√£o do dataset
- Carregamento e an√°lise dos dados
- Implementa√ß√£o das quest√µes propostas
- Explica√ß√µes detalhadas do que foi feito e por qu√™
- Conclus√µes e li√ß√µes aprendidas

---

**Autor:** Implementa√ß√£o para trabalho acad√™mico  
**Data:** 2025  
**Linguagem:** Python 3.12+
